#!/usr/bin/env python3
"""
Build a seat-1-only bidding table parquet file (V3 - Fast Stats Integrated).

This script creates 'bbo_bt_seat1.parquet' by joining the augmented bidding table
with the high-performance statistics generated by the Cube engine.

Process:
1. Scans bt_augmented (541M rows).
2. Filters for Auctions that don't start with 'p-' (opening bids).
3. Joins with the new stats folder (541M rows) directly on 'index'.
4. Writes out the final seat1 table.

Usage:
    python bbo_bt_build_seat1.py
"""

import argparse
import pathlib
import time
from datetime import datetime
import polars as pl


def _format_elapsed(seconds: float) -> str:
    """Format elapsed time as human-readable string."""
    if seconds < 60:
        return f"{seconds:.1f}s"
    elif seconds < 3600:
        return f"{seconds / 60:.1f}m ({seconds:.0f}s)"
    else:
        return f"{seconds / 3600:.1f}h ({seconds / 60:.0f}m)"


def _log_step(step_name: str, step_start: float, min_seconds: float = 60.0) -> None:
    """Log step time if it exceeds min_seconds."""
    elapsed = time.time() - step_start
    if elapsed >= min_seconds:
        print(f"  [step] {step_name}: {_format_elapsed(elapsed)}")

def build_seat1_table_fast(
    bt_augmented_path: pathlib.Path,
    stats_dir: pathlib.Path,
    output_path: pathlib.Path,
    n_rows: int | None = None,
) -> None:
    print("=" * 60)
    print("Building Seat-1-Only Bidding Table (Fast Stats Integration)")
    print("=" * 60)
    
    if not bt_augmented_path.exists():
        print(f"ERROR: Missing {bt_augmented_path}")
        return
    if not stats_dir.exists():
        print(f"ERROR: Missing stats directory {stats_dir}")
        return

    # 1. Scan augmented table
    print(f"Scanning bt_augmented: {bt_augmented_path}")
    lf = pl.scan_parquet(bt_augmented_path)
    if n_rows:
        lf = lf.head(n_rows)
        print(f"  Debug limit: {n_rows:,} rows")

    # Filter to openers only
    lf = lf.filter(~pl.col("Auction").str.starts_with("p-"))
    
    # 2. Scan stats folder
    print(f"Scanning stats: {stats_dir}")
    stats_lf = pl.scan_parquet(stats_dir / "*.parquet")
    
    # 3. Join on index
    # Both are keyed by global 'index' from augmented table
    lf = lf.join(stats_lf, on="index", how="left")
    
    # 4. Final selection and renaming
    # Downstream expects 'bt_index' and 'matching_deal_count' (from Seat 1)
    # Perform final downcasting of core columns
    lf = lf.rename({
        "index": "bt_index",
        "matching_deal_count_S1": "matching_deal_count"
    }).with_columns([
        pl.col("bt_index").cast(pl.UInt32),
        pl.col("seat").cast(pl.UInt8),
    ])
    
    # Ensure all stats columns that might have been joined are Float32/UInt8
    # (In case the stats folder had legacy Float64 files)
    all_cols = lf.collect_schema().names()
    cast_ops = []
    for c in all_cols:
        if any(x in c for x in ["_mean", "_std"]):
            cast_ops.append(pl.col(c).fill_nan(None).cast(pl.Float32))
        elif any(x in c for x in ["_min", "_max"]):
            cast_ops.append(pl.col(c).fill_nan(None).fill_null(0).cast(pl.UInt8))
    if cast_ops:
        lf = lf.with_columns(cast_ops)
    
    # 5. Write results
    # We produce TWO files:
    # A. The full wide Seat 1 table
    # B. A compact stats-only table for the API Explorer (Completed Auctions only)
    
    stats_output_path = output_path.parent / "bbo_bt_criteria_seat1_df.parquet"
    
    print(f"Streaming Wide Table to {output_path}...")
    print(f"Streaming Compact Stats to {stats_output_path}...")
    
    t0 = time.time()
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    # Wide table sink
    lf.sink_parquet(output_path)
    
    # Compact stats sink (filter to completed auctions and select only stats/index)
    # The API expects 'bt_index' as the key.
    stats_lf = pl.scan_parquet(output_path).filter(pl.col("is_completed_auction"))
    
    # Select index and all stats columns (matching_deal_count, _min, _max, _mean, _std)
    stats_cols = ["bt_index"] + [c for c in stats_lf.collect_schema().names() 
                                if any(x in c for x in ["matching_deal_count", "_S1", "_S2", "_S3", "_S4"])]
    stats_lf.select(stats_cols).sink_parquet(stats_output_path)
    
    elapsed = time.time() - t0
    print(f"Done in {elapsed:.1f}s.")
    print(f"  Wide Table: {output_path.stat().st_size / 1024**2:.1f} MB")
    print(f"  Stats Table: {stats_output_path.stat().st_size / 1024**2:.1f} MB")

def main():
    start_time = time.time()
    start_datetime = datetime.now()
    
    print("=" * 70)
    print("BBO SEAT-1 BIDDING TABLE BUILDER")
    print("=" * 70)
    print(f"Start: {start_datetime.strftime('%Y-%m-%d %H:%M:%S')}")
    print()
    
    parser = argparse.ArgumentParser(description="Build seat-1-only bidding table (Fast)")
    parser.add_argument("--bt-rows", type=int, default=None, help="Limit rows for testing")
    parser.add_argument("--output", type=str, default=None, help="Output path")
    args = parser.parse_args()
    
    data_dir = pathlib.Path("e:/bridge/data/bbo/bidding")
    bt_augmented_path = data_dir / "bbo_bt_augmented.parquet"
    stats_dir = data_dir / "bbo_bt_stats"
    
    if args.output:
        output_path = pathlib.Path(args.output)
    else:
        output_path = data_dir / "bbo_bt_seat1.parquet"
    
    build_seat1_table_fast(
        bt_augmented_path=bt_augmented_path,
        stats_dir=stats_dir,
        output_path=output_path,
        n_rows=args.bt_rows,
    )
    
    # Final summary
    end_datetime = datetime.now()
    total_elapsed = time.time() - start_time
    print()
    print("=" * 70)
    print(f"End:     {end_datetime.strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"Elapsed: {_format_elapsed(total_elapsed)}")
    print("=" * 70)

if __name__ == "__main__":
    main()
